# Bach Chorale Transformer

This project trains a Transformer language model for polyphonic Bach chorales and generates new samples as MIDI and MusicXML.

## Representation Choice (Symbolic)

### Summary
This repo uses a **token-based, event-style symbolic representation** tailored to **fixed 4-part (SATB) chorales**.

Each discrete time step is encoded as a small block:

> `TIME` → `S` → `A` → `T` → `B`

Where each voice token is a **single MIDI pitch** (0–127) and `REST=128` represents silence.

### Token set / vocabulary
The tokenizer is implemented in [src/representation.py](src/representation.py).

Special tokens:

- `PAD = 0` (padding, reserved)
- `BOS = 1` (start of sequence)
- `EOS = 2` (end of sequence)
- `TIME = 3` (start of a new time step)

Note tokens:

- `NOTE(p)` where $p \in [0, 128]$ and `p=128` means REST

Mapping to integer IDs:

- `NOTE(p)` is stored as `id = 4 + p`
- `vocab_size = 4 + (128 + 1) = 133`

### Sequence structure
Given a chorale with $T$ time steps (rows in the CSV), the sequence is:

`BOS`, then for each step $t$:

`TIME, NOTE(S_t), NOTE(A_t), NOTE(T_t), NOTE(B_t)`

then `EOS`.

This produces a single 1D stream for a **causal language model** trained with standard next-token prediction.

### Key design choices (justification)

**Why token-based / event-style (vs. piano-roll)?**

- **Causal generation is natural**: the model learns $p(x_i \mid x_{<i})$.
- **Compact and simple**: one step = 5 tokens, without multi-label pitch sets.
- **Easy decoding** back into a 4-part score, with explicit step boundaries.

**Why an explicit `TIME` delimiter?**

- Makes boundaries unambiguous and decoding robust.
- Helps the model learn a stable grid: each `TIME` means “advance one frame”.

**Why fixed voice order (SATB) and 1 token per voice?**

- Chorales in this dataset are consistently 4-part.
- Fixed ordering encodes voice identity implicitly and avoids permutation ambiguity.

**Why MIDI pitch + REST=128?**

- MIDI pitch is a standard integer scale for symbolic music.
- A dedicated REST value keeps the vocabulary small and avoids separate note-off events.

### Conditioning (current & extensibility)
Current training/generation in this repo is **unconditional**: the model only conditions on past tokens.

If you want conditioning later (e.g., key, meter, a given soprano line), you can prepend/insert control tokens such as `KEY_*`, `METER_*`, or per-step constraints before the voices. This keeps the objective unchanged while enabling controllability.

### Limitations / trade-offs
- Fixed 4-voice structure (not directly applicable to arbitrary polyphony).
- Time grid is assumed by the CSV; expressive timing and overlaps are not represented.
- Durations are implicit via repeated pitches across consecutive steps (see the incremental merging logic in [src/stream_web.py](src/stream_web.py)).

## Route A: From Scratch (this repo)

### 1) Prepare Data
Place CSV files with columns `note0,note1,note2,note3` under:

- `data/train/` (training set)
- `data/valid/` (validation set)

Example:
```
/mnt/win_c/bachgen/data/train/chorale_000.csv
```

### 2) Train
`src/train.py` will use **validation NLL (cross-entropy)** as the model selection criterion and save a `*_best.pt` checkpoint.

```
python -m src.train --data-dir data --out-dir runs/from_scratch
```

### 3) Generate (offline, non-streaming)
```
python -m src.generate --checkpoint runs/from_scratch/model_epoch_20.pt --out-dir outputs
```

Outputs:
- `outputs/sample_000.mid`
- `outputs/sample_000.musicxml`

### 4) Generate (streaming, optional)
If you still want the live/streaming UI later, use:

```
python -m src.stream_web
```

## Sample Outputs

Below are three sample chorales generated by the trained model, exported as MusicXML scores:

### Music21 Fragment
![Music21](assets/sample_001.png)
A 4-part SATB chorale with clear voice leading and harmonic progression. The model generates soprano, alto, tenor, and bass voices independently while respecting structural constraints.

### Music21 Fragment (Continued)
![Music21-2](assets/sample_002.png)
Demonstration of longer-form generation with smooth voice transitions and maintained polyphonic texture throughout multiple measures.

### Music21 Fragment (Conclusion)
![Music21-3](assets/sample_003.png)
Final segment showing model's ability to maintain coherent harmonic language and resolve cadences appropriately across all four voices.

**Generation Parameters Used:**
- `--max-timesteps 400` (≈400 time steps per voice)
- `--temperature 1.0`
- `--top-k 32`

These samples were generated during inference using segmented generation with 10-token overlap between segments to maintain context quality across longer sequences.

## Route B: Fine-Tune a Pretrained Model (two options)

### Option B1: Symbolic, GPT-2 as a pretrained LM baseline
Use the same tokenization from this repo, but initialize a `transformers` causal LM from a pretrained `gpt2` checkpoint, resize embeddings to the chorale vocab, and fine-tune on the chorale tokens. This is a fast, practical fine-tuning baseline when no music-specific checkpoints are available.

High-level steps:
1. Convert CSVs to token sequences (reuse `src/representation.py`).
2. Use Hugging Face `AutoModelForCausalLM.from_pretrained("gpt2")`.
3. Resize token embeddings to `vocab_size`.
4. Fine-tune with standard causal LM loss.

### Option B2: Music-specific pretrained model (recommended if you have compute)
For SOTA-style fine-tuning on symbolic music, you can use a music-specific model (e.g., Music Transformer or REMI-based checkpoints). If you have access to a checkpoint, plug in its tokenizer and finetune on chorales by mapping to its tokenization (usually REMI or CP). This can improve musical coherence over a general LM baseline.



## Notes
- Export to MusicXML uses `music21`, so the result opens cleanly in MuseScore.
- You can adjust `temperature` / `top_k` during generation to trade off diversity vs. stability.
